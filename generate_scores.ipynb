{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1df9eb4e-4a94-4322-a54c-69f65729c99c",
   "metadata": {},
   "source": [
    "# Generating RRI Scores\n",
    "2024-08-22\n",
    "\n",
    "This notebook calculates the RRI score based on incident data provided from Mark Thompson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f87680-bae1-4612-bfde-975061770f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import ujson \n",
    "import json\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0984f035-f17a-429f-8736-19f338336f67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anish\\AppData\\Local\\Temp\\ipykernel_38492\\1308895508.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  incident_data['severity'].replace({0: 1}, inplace=True)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp\\ipykernel_38492\\1308895508.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  incident_data['reach'].replace({0: 1}, inplace=True)\n",
      "C:\\Users\\anish\\AppData\\Local\\Temp\\ipykernel_38492\\1308895508.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  incident_data['novelty'].replace({0: 1}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# ISIN to RR ID\n",
    "rr_isin = pd.read_csv('data/REPRISK_ISINs.csv', quoting=csv.QUOTE_NONE)\n",
    "rr_isin.columns = ['rr', 'ISIN']\n",
    "rr_isin['ISIN'] = rr_isin['ISIN'].str.replace('\\\"', '')\n",
    "map_isin = rr_isin.set_index('ISIN')['rr'].to_dict()\n",
    "\n",
    "# Incident Data\n",
    "incident_data = pd.read_csv('data/REPRISK_EVENTS.csv', quoting=csv.QUOTE_NONE)\n",
    "incident_data.columns = ['date', 'ID_incident', 'ID_RR', 'reach', 'severity', 'unsharp', 'novelty', \n",
    "                         'environment', 'social', 'government', 'crosscutting']\n",
    "\n",
    "incident_data['date'] = pd.to_datetime(incident_data['date'])\n",
    "incident_data['severity'].replace({0: 1}, inplace=True)\n",
    "incident_data['reach'].replace({0: 1}, inplace=True)\n",
    "incident_data['novelty'].replace({0: 1}, inplace=True)\n",
    "\n",
    "severity_weights = {1: 1, \n",
    "                    2: 10, \n",
    "                    3: 100}\n",
    "reach_weights = {1: 1, \n",
    "                 2: 2, \n",
    "                 3: 3}\n",
    "novelty_weights = {1: 1,\n",
    "                   2: 2}\n",
    "\n",
    "incident_data[\"Incident Score\"] = (incident_data['severity'].replace(severity_weights)\n",
    "                                   *incident_data['reach'].replace(reach_weights)\n",
    "                                   *incident_data['novelty'].replace(novelty_weights))\n",
    "\n",
    "n_years = 2\n",
    "n_days = 365*n_years\n",
    "curvature_ = 1\n",
    "weights = np.arange(1,n_days + 1)\n",
    "weights = weights/weights.max()\n",
    "weights = ((2**weights) - 1)**curvature_\n",
    "time_weights = pd.Series(weights)\n",
    "\n",
    "start = pd.Timestamp(\"2007-01-01\")\n",
    "todate = max(incident_data['date']).strftime('%Y-%m-%d')\n",
    "end = pd.Timestamp(todate)\n",
    "date_range_extended = pd.date_range(start=start-pd.Timedelta(days=n_days), end=end)\n",
    "date_range = pd.date_range(start=start, end=end)\n",
    "incident_data.set_index(\"ID_RR\", inplace=True)\n",
    "\n",
    "def get_raw_scores(id_, incident_data, col=\"Incident Score\"):\n",
    "    try:\n",
    "        company_incidents = incident_data.reset_index().set_index('date')\n",
    "        company_incidents = company_incidents[company_incidents['ID_RR']==id_]\n",
    "        incident_scores = company_incidents[col].groupby(level=0).sum().reindex(date_range_extended).fillna(0)\n",
    "        stacked_incident_score = incident_scores.rolling(n_days).apply(lambda scores: np.dot(scores, weights)).fillna(0)\n",
    "        stacked_incident_score = stacked_incident_score.reindex(date_range)\n",
    "    except KeyError:\n",
    "        stacked_incident_score = pd.Series(0, index=date_range)\n",
    "    stacked_incident_score.name = \"Incident Score\"\n",
    "    return stacked_incident_score  \n",
    "\n",
    "def scale(series, lambda_=0.000105, curvature=5.3):\n",
    "    return 100*((1 - np.exp(-lambda_*series))**(1/curvature))\n",
    "\n",
    "scaling = scale(pd.Series(range(750)))\n",
    "\n",
    "def max_decay(series, decay=0.5**(1/365)):\n",
    "    new_series = pd.Series(index=series.index, dtype=\"float\")\n",
    "    previous = 0\n",
    "    for date, value in series.items():\n",
    "        if previous > 0:\n",
    "            if value/previous < decay:\n",
    "                new_series[date] = previous*decay\n",
    "            else:\n",
    "                new_series[date] = value\n",
    "        else:\n",
    "            new_series[date] = value\n",
    "            \n",
    "        previous = new_series[date]\n",
    "    return new_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e8c1b3f-d0bf-47a1-8842-2079895e1f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7046850ed9d84c41b5fe8568ef121c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6323 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09d7fe231a94fb992f557a9c5ed200a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6364 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# read spx historical constituents\n",
    "const = pd.read_csv(\"esg_data/spx_constituents.csv\", quoting=csv.QUOTE_NONE, delimiter=' ')\n",
    "const.columns = ['date'] + [x.replace('\\\"', '') for x in const.columns[1:]]\n",
    "const['date'] = const['date'].str.replace('\\\"', '')\n",
    "const.set_index('date', inplace=True)\n",
    "\n",
    "# map each date to its spx constituents\n",
    "spx_const = {}\n",
    "\n",
    "for i in tqdm(range(len(const))):\n",
    "    row = const.iloc[i]\n",
    "    date = const.index[i]\n",
    "    spx_const[date] = set()\n",
    "    for isin in const.columns:\n",
    "        if row[isin] == 1:\n",
    "            spx_const[date].add(isin)\n",
    "\n",
    "# all the spx isin constituents ever\n",
    "all_spx_isin = set(const.columns)\n",
    "\n",
    "# read historical euro constituents list\n",
    "const = pd.read_csv(\"esg_data/stoxx_constituents.csv\", quoting=csv.QUOTE_NONE, delimiter=' ')\n",
    "const.columns = ['date'] + [x.replace('\\\"', '') for x in const.columns[1:]]\n",
    "const['date'] = const['date'].str.replace('\\\"', '')\n",
    "const.set_index('date', inplace=True)\n",
    "\n",
    "# map each date to its eurostoxx constituents\n",
    "euro_const = {}\n",
    "\n",
    "for i in tqdm(range(len(const))):\n",
    "    row = const.iloc[i]\n",
    "    date = const.index[i]\n",
    "    euro_const[date] = set()\n",
    "    for isin in const.columns:\n",
    "        if row[isin] == 1:\n",
    "            euro_const[date].add(isin)\n",
    "\n",
    "all_euro_isin = set(const.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641820c5-171b-4099-bde3-e5df5507cbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_esg = {}\n",
    "\n",
    "# calculate and plot ESG sub-scores and overall score for each isin in SPX\n",
    "esgx = [\"environment\", \"social\", \"government\", \"crosscutting\"]\n",
    "for isin in tqdm(all_spx_isin.intersection(set(map_isin.keys()))):\n",
    "    if len(spx_esg.get(isin, [])):\n",
    "        continue\n",
    "    id_ = map_isin[isin]\n",
    "    scores = {}\n",
    "    \n",
    "    series = get_raw_scores(id_, incident_data)\n",
    "    series = max_decay(scale(series))\n",
    "    scores['esg_score'] = series\n",
    "    \n",
    "    for subscore in esgx:\n",
    "        series = get_raw_scores(id_, incident_data[incident_data[subscore] == 1])\n",
    "        series = max_decay(scale(series))\n",
    "        scores[subscore] = series\n",
    "    \n",
    "    spx_esg[isin] = pd.DataFrame(scores)\n",
    "\n",
    "for isin in tqdm(all_spx_isin.intersection(set(map_isin.keys()))):\n",
    "    spx_esg[isin] = spx_esg[isin].to_dict(orient='index')\n",
    "\n",
    "def convert_timestamp_keys(d):\n",
    "    for isin, nested_dict in tqdm(d.items()):\n",
    "        d[isin] = {str(timestamp): inner_dict for timestamp, inner_dict in nested_dict.items()}\n",
    "    return d\n",
    "\n",
    "spx_esg_serializable = convert_timestamp_keys(spx_esg)\n",
    "\n",
    "with open('spx_esg.json', 'w') as json_file:\n",
    "    ujson.dump(spx_esg_serializable, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31e9fe2-2b77-4c3e-bcac-20ac35cd81f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "euro_esg = {}\n",
    "\n",
    "# calculate and plot ESG sub-scores and overall score for each isin in Eurostoxx 600\n",
    "esgx = [\"environment\", \"social\", \"government\", \"crosscutting\"]\n",
    "for isin in tqdm(all_euro_isin.intersection(set(map_isin.keys()))):\n",
    "    if len(euro_esg.get(isin, [])):\n",
    "        continue\n",
    "    id_ = map_isin[isin]\n",
    "    scores = {}\n",
    "    \n",
    "    series = get_raw_scores(id_, incident_data)\n",
    "    series = max_decay(scale(series))\n",
    "    scores['esg_score'] = series\n",
    "    \n",
    "    for subscore in esgx:\n",
    "        series = get_raw_scores(id_, incident_data[incident_data[subscore] == 1])\n",
    "        series = max_decay(scale(series))\n",
    "        scores[subscore] = series\n",
    "    \n",
    "    euro_esg[isin] = pd.DataFrame(scores)\n",
    "\n",
    "for isin in tqdm(all_euro_isin.intersection(set(map_isin.keys()))):\n",
    "    euro_esg[isin] = euro_esg[isin].to_dict(orient='index')\n",
    "\n",
    "def convert_timestamp_keys(d):\n",
    "    for isin, nested_dict in tqdm(d.items()):\n",
    "        d[isin] = {str(timestamp): inner_dict for timestamp, inner_dict in nested_dict.items()}\n",
    "    return d\n",
    "\n",
    "euro_esg_serializable = convert_timestamp_keys(euro_esg)\n",
    "\n",
    "with open('euro_esg.json', 'w') as json_file:\n",
    "    ujson.dump(euro_esg_serializable, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f842f819-5a60-4068-9052-b4c345b03dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE = \"bond_data/historical.constituents_iBoxx_USD_HIGH_YIELD_COMPONENTS.csv\"\n",
    "FILE = \"bond_data/historical.constituents_iBoxx_USD_INVESTMENT_GRADE_COMPONENTS.csv\" # bond dtta US investment grade location\n",
    "# FILE = \"bond_data/historical.constituents_iBoxx_EUR_HIGH_YIELD_COMPONENTS.csv\"\n",
    "\n",
    "df = pd.read_csv(FILE, quoting=csv.QUOTE_NONE) # convert to pandas dataframe\n",
    "\n",
    "df.columns = ['date', 'isin', 'index_weight', 'bid_price', 'ask_price']\n",
    "\n",
    "for key in ['date', 'isin', 'index_weight', 'bid_price', 'ask_price']:\n",
    "    df[key] = df[key].str.replace('\\\"', '')\n",
    "    if key not in {'date', 'isin'}:\n",
    "        df[key] = df[key].astype(float)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date']) # set date as datetime object and make it the index\n",
    "df.set_index('date', inplace=True)\n",
    "us_bonds_const = {} # dictionary for bond constituents for each date\n",
    "for date in tqdm(set(df.index)):\n",
    "    us_bonds_const[date] = df[df.index == date]\n",
    "\n",
    "dates = sorted(list(set(df.index))) # sort given dates (monthly)\n",
    "\n",
    "us_bond_esg = {}\n",
    "all_us_bond_isin = set(df['isin'])\n",
    "\n",
    "s = set()\n",
    "for isin in all_us_bond_isin.intersection(set(map_isin.keys())):\n",
    "    s.add(map_isin[isin])\n",
    "\n",
    "esgx = [\"environment\", \"social\", \"government\", \"crosscutting\"]\n",
    "for id_ in tqdm(s):\n",
    "    if len(us_bond_esg.get(id_, [])):\n",
    "        continue\n",
    "    scores = {}\n",
    "    \n",
    "    series = get_raw_scores(id_, incident_data)\n",
    "    series = max_decay(scale(series))\n",
    "    scores['esg_score'] = series\n",
    "    \n",
    "    for subscore in esgx:\n",
    "        series = get_raw_scores(id_, incident_data[incident_data[subscore] == 1])\n",
    "        series = max_decay(scale(series))\n",
    "        scores[subscore] = series\n",
    "    \n",
    "    us_bond_esg[id_] = pd.DataFrame(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
